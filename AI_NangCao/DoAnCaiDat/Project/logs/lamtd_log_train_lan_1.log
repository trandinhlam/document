
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 50, 50, 64)        1792      
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 25, 25, 64)        0         
_________________________________________________________________
dropout (Dropout)            (None, 25, 25, 64)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 23, 23, 64)        36928     
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 11, 11, 64)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 11, 11, 64)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 9, 9, 64)          36928     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 4, 4, 64)          0         
_________________________________________________________________
flatten (Flatten)            (None, 1024)              0         
_________________________________________________________________
dense (Dense)                (None, 128)               131200    
_________________________________________________________________
dropout_3 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 43)                5547      
=================================================================
Total params: 212,395
Trainable params: 212,395
Non-trainable params: 0
_________________________________________________________________
None

TapTrain: 0.8, táº­p test 0.3

#start train
model.fit(x=x_train, y=y_train,
          epochs=25,
          batch_size=64,
          validation_data=(x_val, y_val),
          callbacks=[early_stopping],
          verbose=2
          )
model.save('Model.h5')


2021-03-20 08:29:27.535164: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 941010000 exceeds 10% of free system memory.
2021-03-20 08:29:28.586187: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-20 08:29:28.610681: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2699905000 Hz
2021-03-20 08:29:29.163328: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 40960000 exceeds 10% of free system memory.
2021-03-20 08:29:29.307889: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 25595136 exceeds 10% of free system memory.
2021-03-20 08:29:29.376102: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 20480000 exceeds 10% of free system memory.
2021-03-20 08:29:29.376146: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 40960000 exceeds 10% of free system memory.

Epoch 1/25
491/491 - 116s - loss: 2.7961 - accuracy: 0.2235 - val_loss: 1.6406 - val_accuracy: 0.5201
Epoch 2/25
491/491 - 120s - loss: 1.4562 - accuracy: 0.5325 - val_loss: 0.6761 - val_accuracy: 0.8187
Epoch 3/25
491/491 - 150s - loss: 0.8513 - accuracy: 0.7203 - val_loss: 0.3157 - val_accuracy: 0.9221
Epoch 4/25
491/491 - 189s - loss: 0.6040 - accuracy: 0.8010 - val_loss: 0.2320 - val_accuracy: 0.9462
Epoch 5/25
491/491 - 112s - loss: 0.4953 - accuracy: 0.8358 - val_loss: 0.1537 - val_accuracy: 0.9592
Epoch 6/25
491/491 - 108s - loss: 0.4078 - accuracy: 0.8676 - val_loss: 0.1199 - val_accuracy: 0.9741
Epoch 7/25
491/491 - 108s - loss: 0.3735 - accuracy: 0.8792 - val_loss: 0.1059 - val_accuracy: 0.9770
Epoch 8/25
491/491 - 109s - loss: 0.3181 - accuracy: 0.8944 - val_loss: 0.0779 - val_accuracy: 0.9823
Epoch 9/25
491/491 - 106s - loss: 0.2999 - accuracy: 0.9021 - val_loss: 0.0701 - val_accuracy: 0.9838
Epoch 10/25
491/491 - 108s - loss: 0.2813 - accuracy: 0.9105 - val_loss: 0.0653 - val_accuracy: 0.9858
Epoch 11/25
491/491 - 145s - loss: 0.2530 - accuracy: 0.9199 - val_loss: 0.0539 - val_accuracy: 0.9864
Epoch 12/25
491/491 - 119s - loss: 0.2485 - accuracy: 0.9202 - val_loss: 0.0560 - val_accuracy: 0.9875
Epoch 13/25
491/491 - 107s - loss: 0.2288 - accuracy: 0.9268 - val_loss: 0.0445 - val_accuracy: 0.9878
Epoch 14/25
491/491 - 108s - loss: 0.2258 - accuracy: 0.9279 - val_loss: 0.0450 - val_accuracy: 0.9908
Epoch 15/25
491/491 - 109s - loss: 0.2110 - accuracy: 0.9333 - val_loss: 0.0363 - val_accuracy: 0.9920
Epoch 16/25
491/491 - 112s - loss: 0.1988 - accuracy: 0.9372 - val_loss: 0.0398 - val_accuracy: 0.9887
Epoch 17/25
491/491 - 115s - loss: 0.2000 - accuracy: 0.9376 - val_loss: 0.0355 - val_accuracy: 0.9917
Epoch 18/25
491/491 - 109s - loss: 0.1927 - accuracy: 0.9387 - val_loss: 0.0314 - val_accuracy: 0.9930
Epoch 19/25
491/491 - 108s - loss: 0.1865 - accuracy: 0.9404 - val_loss: 0.0295 - val_accuracy: 0.9939
Epoch 20/25
491/491 - 108s - loss: 0.1890 - accuracy: 0.9405 - val_loss: 0.0293 - val_accuracy: 0.9935
Epoch 21/25
491/491 - 108s - loss: 0.1768 - accuracy: 0.9442 - val_loss: 0.0266 - val_accuracy: 0.9930
Epoch 22/25
491/491 - 108s - loss: 0.1795 - accuracy: 0.9464 - val_loss: 0.0258 - val_accuracy: 0.9938
Epoch 23/25
491/491 - 107s - loss: 0.1733 - accuracy: 0.9453 - val_loss: 0.0259 - val_accuracy: 0.9930
Epoch 24/25
491/491 - 107s - loss: 0.1729 - accuracy: 0.9458 - val_loss: 0.0266 - val_accuracy: 0.9940

Process finished with exit code 0

              precision    recall  f1-score   support

           0       0.98      1.00      0.99        60
           1       0.99      0.99      0.99       720
           2       0.98      1.00      0.99       750
           3       1.00      0.93      0.96       450
           4       1.00      0.98      0.99       660
           5       0.95      0.97      0.96       630
           6       1.00      0.93      0.96       150
           7       0.96      0.96      0.96       450
           8       0.95      0.98      0.96       450
           9       0.94      1.00      0.97       480
          10       1.00      0.98      0.99       660
          11       0.94      1.00      0.97       420
          12       1.00      0.99      0.99       690
          13       1.00      0.99      1.00       720
          14       1.00      1.00      1.00       270
          15       0.99      1.00      0.99       210
          16       0.99      1.00      1.00       150
          17       1.00      0.93      0.97       360
          18       0.99      0.92      0.95       390
          19       1.00      1.00      1.00        60
          20       0.84      1.00      0.91        90
          21       0.77      0.83      0.80        90
          22       0.99      0.97      0.98       120
          23       0.99      0.99      0.99       150
          24       0.97      0.99      0.98        90
          25       0.95      0.95      0.95       480
          26       0.95      0.88      0.92       180
          27       0.91      0.50      0.65        60
          28       0.99      0.98      0.99       150
          29       0.76      0.99      0.86        90
          30       0.99      0.89      0.94       150
          31       0.94      0.99      0.97       270
          32       1.00      1.00      1.00        60
          33       0.98      1.00      0.99       210
          34       0.99      0.99      0.99       120
          35       0.99      1.00      0.99       390
          36       0.99      0.97      0.98       120
          37       1.00      1.00      1.00        60
          38       0.98      1.00      0.99       690
          39       0.98      0.99      0.98        90
          40       0.94      0.98      0.96        90
          41       1.00      0.80      0.89        60
          42       0.97      1.00      0.98        90


                precision    recall  f1-score   support
    accuracy                           0.97     12630
   macro avg       0.97      0.96      0.96     12630
weighted avg       0.98      0.97      0.97     12630


Process finished with exit code 0
